{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top three blocks of code are composed of two sub-functions and one function to calculate the sum of the Spearman Distances between a proposed ranking and a dictionary of existing rankings.\n",
    "\n",
    "The final block of code is a short unit-test to check that each function is working as intended.\n",
    "\n",
    "There are several assumptions (as noted in the code comments) about the quality of the input variables and how the numbers should be interpreted. For example, we assume that all items were ranked by all metrics. We also assume that higher rankings are better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spearmanDistance(rankA, rankB):\n",
    "    \"\"\" Compute the Spearman Distance between RankA and RankB. \n",
    "    \n",
    "        The Spearman Distance is the sum of the absolute values of the \n",
    "        difference in ranking for each item in the list.\n",
    "        \n",
    "        We're assuming that both lists contain the same elements,\n",
    "        otherwise this will break.\n",
    "        \n",
    "        @RankA List containing the same elements as RankB\n",
    "        @RankB List containing the same elements as RankA\n",
    "        @return The Spearman Distance between RankA and RankB\n",
    "    \"\"\"             \n",
    "    sum = 0\n",
    "    for i in range(len(rankA)):\n",
    "        sum += abs(i - rankB.index(rankA[i]))\n",
    "    return sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRanking(scores, metric_number):\n",
    "    \"\"\"\n",
    "        Compute a listed ranking using a particular metric for items in scores.\n",
    "        \n",
    "        For this we are assuming that higher scores are better and should be at\n",
    "        the front of the list. This can be changed by setting reverse to False.\n",
    "        \n",
    "        @scores A dict of {itemId: tuple of scores}\n",
    "        @metric_number The element in the tuple to use for ranking.\n",
    "        @return A list of items ordered by the metric specified.\n",
    "    \"\"\"\n",
    "    return sorted(scores, key=lambda item: scores.get(item)[metric_number], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sumSpearmanDistances(scores, proposedRanking):\n",
    "    \"\"\"Calculate the sum of Spearman’s Footrule Distances for a given proposedRanking.\n",
    "    scores : A dict of {itemId: tuple of scores} \n",
    "        e.g. {‘A’: [100, 0.1], ‘B’: [90, 0.3], ‘C’: [20, 0.2]} \n",
    "        means that item ‘A’ was given a score of 100 by metric 1\n",
    "        and a score of 0.1 by metric 2 etc\n",
    "    proposedRanking : An ordered list of itemIds where the first \n",
    "        entry is the proposed-best and last entry is the proposed \n",
    "        worst e.g. [‘A’, ‘B’, ‘C’]\n",
    "    \"\"\"\n",
    "    num_metrics = len(list(scores.values())[0])\n",
    "    sum = 0\n",
    "    for metric in range(num_metrics):\n",
    "        metric_ranking = getRanking(scores, metric)\n",
    "        sum += spearmanDistance(proposedRanking, metric_ranking)\n",
    "    return sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below this is some basic unit testing to verify each of the functions works as intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_distance_bigger_swap (__main__.SpearmanDistanceTestCase) ... ok\n",
      "test_distance_rotate (__main__.SpearmanDistanceTestCase) ... ok\n",
      "test_distance_same (__main__.SpearmanDistanceTestCase) ... ok\n",
      "test_distance_swap (__main__.SpearmanDistanceTestCase) ... ok\n",
      "test_get_ranking_0 (__main__.SpearmanDistanceTestCase) ... ok\n",
      "test_get_ranking_1 (__main__.SpearmanDistanceTestCase) ... ok\n",
      "test_sum_spearman (__main__.SpearmanDistanceTestCase) ... ok\n",
      "test_sum_spearman_simple (__main__.SpearmanDistanceTestCase) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 8 tests in 0.052s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7ff94c2b1fd0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "class SpearmanDistanceTestCase(unittest.TestCase):\n",
    "    a = ['A', 'B', 'C', 'D']\n",
    "    b = ['B', 'A', 'C', 'D']\n",
    "    c = ['C', 'B', 'A', 'D']\n",
    "    d = ['B', 'C', 'A', 'D']\n",
    "    scores = {'A': [100, 0.1], 'B': [90, 0.3], 'C': [20, 0.2], 'D': [5, 0.05]}\n",
    "\n",
    "    # Tests for calculating spearman distance between two lists\n",
    "    def test_distance_same(self):\n",
    "        self.assertEqual(spearmanDistance(self.a, self.a), 0)\n",
    "    def test_distance_swap(self):\n",
    "        self.assertEqual(spearmanDistance(self.a, self.b), 2)\n",
    "    def test_distance_bigger_swap(self):\n",
    "        self.assertEqual(spearmanDistance(self.a, self.c), 4)    \n",
    "    def test_distance_rotate(self):\n",
    "        self.assertEqual(spearmanDistance(self.a, self.d), 4)    \n",
    "        \n",
    "    # Tests of extracting a ranked list from the score format\n",
    "    def test_get_ranking_0(self):\n",
    "        self.assertEqual(getRanking(self.scores, 0), self.a)\n",
    "    def test_get_ranking_1(self):\n",
    "        self.assertEqual(getRanking(self.scores, 1), self.d)\n",
    "\n",
    "    # Tests of the whole summation calculation\n",
    "    def test_sum_spearman_simple(self):\n",
    "        self.assertEqual(sumSpearmanDistances(self.scores, self.a), 4)\n",
    "    def test_sum_spearman(self):\n",
    "        r1 = getRanking(self.scores, 0)\n",
    "        r2 = getRanking(self.scores, 0)\n",
    "        sum = spearmanDistance(self.b, r1) + spearmanDistance(self.b, r1)\n",
    "        self.assertEqual(sumSpearmanDistances(self.scores, self.b), sum)\n",
    "\n",
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
